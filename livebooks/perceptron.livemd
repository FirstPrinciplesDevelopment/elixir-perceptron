<!-- livebook:{"persist_outputs":true} -->

# Perceptron

## Install nx

```elixir
Mix.install([
  {:nx, "~> 0.1"}
])
```

<!-- livebook:{"output":true} -->

```
Resolving Hex dependencies...
Dependency resolution completed:
New:
  nx 0.1.0
* Getting nx (Hex package)
==> nx
Compiling 23 files (.ex)
Generated nx app
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Definitions And Theory

$$r$$: learning rate, a constant chosen between 0 and 1.

$$\vec{w}$$: the weight vector.

$$w_{i}$$: the $$i$$th weight in the weight vector.

$$w_{i^{\prime}}$$: the updated $$i$$th weight in the weight vector.

$$\vec{x}$$: the input vector.

$$x_{i}$$: the $$i$$th input in the input vector.

$$d$$: the desired, correct output for the input vector.

$$y$$: the actual output for the input vector.

### Making Predictions: Feed Forward

The perceptron has an initial of weight vector $$\vec{w}$$, which is used to make a binary classification prediction $$y$$ on an input vector $$\vec{x}$$:

$$y = \begin{cases}1 & \text{if }\ \vec{w}\cdot\vec{x} > 0,\\0 & \text{otherwise}\end{cases}$$

### Learning: Updating Weights

After each feed-forward step, the perceptron *learns* by updating each weight $$w_{i}$$ in the weight vector $$\vec{w}$$. 
The updated weight is denoted $$w_{i^{\prime}}$$, and is calculated as follows:

$$w_{i^{\prime}} = w_{i} + r(d - y)x_{i}$$.

## Perceptron Implementation

```elixir
defmodule Perceptron do
  @doc """
  Returns an initialized weights vector with `length` elements.

  ## Examples

      iex> init_weights(3)
      %Nx.Tensor{...}
  """
  def init_weights(length) do
    Nx.random_normal({length}, 0.0, 0.0)
  end

  @doc """
  Trains `weights` on a list of input vectors, `data`, with given learning rate `r`.

  Returns the updated weight vector.

  ## Examples

      iex> train([{input, d}, {input, d}], r, weights)
      %Nx.Tensor{...}
  """
  def train(data, r, weights \\ nil) do
    # base case
    if length(data) == 1 do
      train_single(hd(data), r, weights)
    else
      weights = train_single(hd(data), r, weights)
      train(tl(data), r, weights)
    end
  end

  @doc """
  Trains `weights` on a single input, with given learning rate `r`.

  Returns the updated weight vector.

  ## Examples

      iex> train_single({input, d}, r, weights)
      %Nx.Tensor{...}
  """
  def train_single({%Nx.Tensor{} = input, d}, r, weights \\ nil) do
    weights =
      case weights do
        nil -> init_weights(Nx.size(input))
        _ -> weights
      end

    y = feed_forward(weights, input)
    update_weights(weights, input, r, d, y)
  end

  @doc """
  Tests `weights` ability to correctly classify a single `input` vector.

  Returns `true` if the output is equal to the desired output, `d`, `false` otherwise

  ## Examples
      iex> test_single({input, d}, weights)
      true
  """
  def test_single({%Nx.Tensor{} = input, d}, %Nx.Tensor{} = weights) do
    y = feed_forward(weights, input)
    d == y
  end

  @doc """
  Classifies an `input` vector based on a `weights` vector.

  Returns a binary value (0 or 1) representing the the predicted class.

  ## Examples
      iex> feed_forwared(weights, input)
      1
  """
  def feed_forward(%Nx.Tensor{} = weights, %Nx.Tensor{} = input) do
    if Nx.to_number(Nx.dot(weights, input)) > 0, do: 1, else: 0
  end

  def normalize(value) do
    cond do
      value > 0 -> 1
      value <= 0 -> 0
    end
  end

  def update_weights(%Nx.Tensor{} = weights, %Nx.Tensor{} = input, r, d, y) do
    Nx.tensor(update_weight_vector(weights, input, r, d, y))
  end

  defp update_weight_vector(%Nx.Tensor{} = weights, %Nx.Tensor{} = input, r, d, y) do
    if Nx.size(weights) == 1 do
      [update_weight(Nx.to_number(weights[0]), Nx.to_number(input[0]), r, d, y)]
    else
      [update_weight(Nx.to_number(weights[0]), Nx.to_number(input[0]), r, d, y)] ++
        update_weight_vector(weights[1..-1//1], input[1..-1//1], r, d, y)
    end
  end

  defp update_weight(w, x, r, d, y) do
    w + r * (d - y) * x
    # Nx.add(w, Nx.multiply(r, Nx.multiply((Nx.subtract(d, y)), x)))
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Perceptron, <<70, 79, 82, 49, 0, 0, 20, ...>>, {:update_weight, 5}}
```

## Prepare Training Input

The perceptron can learn a simple function such as logical AND ($$ \land $$). 
Here is a truth table of the input we will use, including a `bias` column that is always 1:

| `bias` | $$x_{1}$$ | $$x_{2}$$ | $$x_{1} \land x_{2}$$ |
| ------ | --------- | --------- | --------------------- |
| 1      | 0         | 0         | 0                     |
| 1      | 1         | 0         | 0                     |
| 1      | 0         | 1         | 0                     |
| 1      | 1         | 1         | 1                     |

```elixir
training_data = [
  {Nx.tensor([1, 0, 0]), 0},
  {Nx.tensor([1, 1, 0]), 0},
  {Nx.tensor([1, 0, 1]), 0},
  {Nx.tensor([1, 1, 1]), 1},
  {Nx.tensor([1, 0, 0]), 0},
  {Nx.tensor([1, 1, 0]), 0},
  {Nx.tensor([1, 0, 1]), 0},
  {Nx.tensor([1, 1, 1]), 1},
  {Nx.tensor([1, 0, 1]), 0},
  {Nx.tensor([1, 1, 1]), 1},
  {Nx.tensor([1, 1, 1]), 1}
]

learning_rate = 0.5

weights = Perceptron.train(training_data, learning_rate)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[3]
  [-0.5, 0.5, 0.5]
>
```

```elixir
# weights = Nx.tensor([-1, 0.75, 0.75])
for data <- training_data do
  IO.puts(Perceptron.test_single(data, weights))
end
```

<!-- livebook:{"output":true} -->

```
true
true
true
true
true
true
true
true
true
true
true
```

<!-- livebook:{"output":true} -->

```
[:ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok, :ok]
```
